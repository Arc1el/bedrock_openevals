{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 환경변수에서 값을 가져와 설정\n",
    "os.environ['LANGSMITH_TRACING'] = os.getenv('LANGSMITH_TRACING', 'true')\n",
    "os.environ['LANGSMITH_API_KEY'] = os.getenv('LANGSMITH_API_KEY', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client, wrappers\n",
    "from openevals.llm import create_llm_as_judge\n",
    "from openevals.prompts import CORRECTNESS_PROMPT\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "\n",
    "sonnet = ChatBedrockConverse(\n",
    "    model=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "    region_name=\"us-east-1\",\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from assets.evaluation_prompt import (\n",
    "    completeness,\n",
    "    correctness,\n",
    "    helpfulness,\n",
    "    following_instructions,\n",
    "    logical_coherence,\n",
    "    professional_tone,\n",
    "    relevance,\n",
    "    stereotyping,\n",
    "    harmfulness,\n",
    "    refusal,\n",
    "    faithfulness,\n",
    ")\n",
    "\n",
    "\n",
    "# 올바른 시그니처를 가진 evaluator들\n",
    "def logical_coherence_evaluator(inputs: dict, outputs: dict, reference_outputs: dict):\n",
    "    eval = create_llm_as_judge(\n",
    "        prompt=logical_coherence.sonnet_3_7,\n",
    "        model=\"bedrock_converse:apac.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "        feedback_key=\"logical_coherence\",\n",
    "        continuous=True\n",
    "    )\n",
    "    eval_result = eval(\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        reference_outputs=reference_outputs\n",
    "    )\n",
    "    return eval_result\n",
    "def faithfulness_evaluator(inputs: dict, outputs: dict, reference_outputs: dict):\n",
    "    eval = create_llm_as_judge(\n",
    "        prompt=faithfulness.sonnet_3_7,\n",
    "        model=\"bedrock_converse:apac.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "        feedback_key=\"faithfulness\",\n",
    "        continuous=True\n",
    "    )\n",
    "    eval_result = eval(\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        reference_outputs=reference_outputs\n",
    "    )\n",
    "    return eval_result\n",
    "def following_instructions_evaluator(inputs: dict, outputs: dict, reference_outputs: dict):\n",
    "    eval = create_llm_as_judge(\n",
    "        prompt=following_instructions.sonnet_3_7,\n",
    "        model=\"bedrock_converse:apac.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "        feedback_key=\"following_instructions\",\n",
    "        choices=[0, 1]\n",
    "    )\n",
    "    eval_result = eval(\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        reference_outputs=reference_outputs\n",
    "    )\n",
    "    return eval_result\n",
    "def completeness_evaluator(inputs: dict, outputs: dict, reference_outputs: dict):\n",
    "    eval = create_llm_as_judge(\n",
    "        prompt=completeness.sonnet_3_7,\n",
    "        model=\"bedrock_converse:apac.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "        feedback_key=\"completeness\",\n",
    "        continuous=True\n",
    "    )\n",
    "    eval_result = eval(\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        reference_outputs=reference_outputs\n",
    "    )\n",
    "    return eval_result\n",
    "def correctness_evaluator(inputs: dict, outputs: dict, reference_outputs: dict):\n",
    "    eval = create_llm_as_judge(\n",
    "        prompt=correctness.sonnet_3_7,\n",
    "        model=\"bedrock_converse:apac.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "        feedback_key=\"correctness\",\n",
    "        continuous=True\n",
    "    )\n",
    "    eval_result = eval(\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        reference_outputs=reference_outputs\n",
    "    )\n",
    "    return eval_result\n",
    "def helpfulness_evaluator(inputs: dict, outputs: dict, reference_outputs: dict):\n",
    "    eval = create_llm_as_judge(\n",
    "        prompt=helpfulness.sonnet_3_7,\n",
    "        model=\"bedrock_converse:apac.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "        feedback_key=\"helpfulness\",\n",
    "        continuous=True\n",
    "    )\n",
    "    eval_result = eval(\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        reference_outputs=reference_outputs\n",
    "    )\n",
    "    return eval_result\n",
    "def professional_tone_evaluator(inputs: dict, outputs: dict, reference_outputs: dict):\n",
    "    eval = create_llm_as_judge(\n",
    "        prompt=professional_tone.sonnet_3_7,\n",
    "        model=\"bedrock_converse:apac.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "        feedback_key=\"professional_tone\",\n",
    "        continuous=True\n",
    "    )\n",
    "    eval_result = eval(\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        reference_outputs=reference_outputs\n",
    "    )\n",
    "    return eval_result\n",
    "def relevance_evaluator(inputs: dict, outputs: dict, reference_outputs: dict):\n",
    "    eval = create_llm_as_judge(\n",
    "        prompt=relevance.sonnet_3_7,\n",
    "        model=\"bedrock_converse:apac.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "        feedback_key=\"relevance\",\n",
    "        continuous=True\n",
    "    )\n",
    "    eval_result = eval(\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        reference_outputs=reference_outputs\n",
    "    )\n",
    "    return eval_result\n",
    "def stereotyping_evaluator(inputs: dict, outputs: dict, reference_outputs: dict):\n",
    "    eval = create_llm_as_judge(\n",
    "        prompt=stereotyping.sonnet_3_7,\n",
    "        model=\"bedrock_converse:apac.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "        feedback_key=\"stereotyping\",\n",
    "        choices=[0, 1]\n",
    "    )\n",
    "    eval_result = eval(\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        reference_outputs=reference_outputs\n",
    "    )\n",
    "    return eval_result\n",
    "def harmfulness_evaluator(inputs: dict, outputs: dict, reference_outputs: dict):\n",
    "    eval = create_llm_as_judge(\n",
    "        prompt=harmfulness.sonnet_3_7,\n",
    "        model=\"bedrock_converse:apac.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "        feedback_key=\"harmfulness\",\n",
    "        choices=[0, 1]\n",
    "    )\n",
    "    eval_result = eval(\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        reference_outputs=reference_outputs\n",
    "    )\n",
    "    return eval_result\n",
    "def refusal_evaluator(inputs: dict, outputs: dict, reference_outputs: dict):\n",
    "    eval = create_llm_as_judge(\n",
    "        prompt=refusal.sonnet_3_7,\n",
    "        model=\"bedrock_converse:apac.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "        feedback_key=\"refusal\",\n",
    "        choices=[0, 1]\n",
    "    )\n",
    "    eval_result = eval(\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        reference_outputs=reference_outputs\n",
    "    )\n",
    "    return eval_result\n",
    "\n",
    "def tps_evaluator(inputs: dict, outputs: dict, reference_outputs: dict) -> dict:\n",
    "    \"\"\"Extract TPS (Tokens Per Second) from outputs.\"\"\"\n",
    "    return {\n",
    "        \"key\": \"tps\", \n",
    "        \"score\": outputs.get(\"tps\", 0)\n",
    "    }\n",
    "\n",
    "def ttft_evaluator(inputs: dict, outputs: dict, reference_outputs: dict) -> dict:\n",
    "    \"\"\"Extract TTFT metric from outputs.\"\"\"\n",
    "    return {\n",
    "        \"key\": \"ttft_ms\", \n",
    "        \"score\": outputs.get(\"ttft_ms\", 0)\n",
    "    }\n",
    "\n",
    "def output_tokens_evaluator(inputs: dict, outputs: dict, reference_outputs: dict) -> dict:\n",
    "    \"\"\"Extract output token count from outputs.\"\"\"\n",
    "    return {\n",
    "        \"key\": \"output_tokens\", \n",
    "        \"score\": outputs.get(\"output_tokens\", 0)\n",
    "    }\n",
    "\n",
    "def input_tokens_evaluator(inputs: dict, outputs: dict, reference_outputs: dict) -> dict:\n",
    "    \"\"\"Extract input token count from outputs.\"\"\"\n",
    "    return {\n",
    "        \"key\": \"input_tokens\", \n",
    "        \"score\": outputs.get(\"input_tokens\", 0)\n",
    "    }\n",
    "\n",
    "def generation_time_evaluator(inputs: dict, outputs: dict, reference_outputs: dict) -> dict:\n",
    "    \"\"\"Extract generation time from outputs.\"\"\"\n",
    "    return {\n",
    "        \"key\": \"generation_time_s\", \n",
    "        \"score\": outputs.get(\"generation_time_s\", 0)\n",
    "    }\n",
    "\n",
    "def cost_evaluator(inputs: dict, outputs: dict, reference_outputs: dict) -> dict:\n",
    "    \"\"\"Extract cost from outputs.\"\"\"\n",
    "    return {\n",
    "        \"key\": \"total_cost\", \n",
    "        \"score\": outputs.get(\"total_cost\", 0)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYou are an expert data labeler evaluating model outputs for completeness. Your task is to check if the candidate response contains the necessary amount of information and details for answering the question.\\n\\nPlease evaluate the completeness of the output based on the following criteria:\\n\\n1. Does the output address all parts of the input\\'s request?\\n2. Is any required information missing?\\n3. For multi-part requests, are all parts fulfilled?\\n4. Is the level of detail appropriate for the task?\\n5. For specific requests (e.g., \"list 10 items\"), does the output meet the exact requirements?\\n6. For summarization or rewriting tasks, are all main points covered?\\n7. For step-by-step instructions, are all necessary steps included?\\n8. Has any important information been omitted in editing or rewriting tasks?\\n\\nSpecial consideration for evasive or \"I don\\'t know\" type responses:\\n- If the output evades responding or claims lack of knowledge, assess whether this response is justified based on the information available in the input.\\n- If the output states there isn\\'t enough information in the context, but there actually is sufficient information, rate it as incomplete.\\n- If there truly isn\\'t enough information in the context to answer the input, and the output acknowledges this, consider it complete.\\n- Always keep in mind the principle of completeness: Does the output contain all of the necessary information and detail for answering the input, given the available information?\\n\\n<Rubric>\\nAssign a score of 0.0, 0.25, 0.5, 0.75, or 1.0 based on the following criteria:\\n- 0.0: None of the necessary information and detail is present.\\n- 0.25: Less than half of the necessary information and detail is present.\\n- 0.5: About half of the necessary information and detail is present, or it\\'s unclear what the right amount of information is.\\n- 0.75: Most of the necessary information and detail is present.\\n- 1.0: All necessary information and detail is present.\\n</Rubric>\\n\\nRemember:\\n- Focus on completeness, not accuracy or truthfulness.\\n- Evaluate whether the output addresses the input, even if the information provided is incorrect.\\n- Consider the appropriate level of detail for the intended audience or specified length.\\n- For evasive responses, evaluate if the evasion is justified given the available information.\\n\\n<input>\\n{inputs}\\n</input>\\n\\n<output>\\n{outputs}\\n</output>\\n\\n<reference_outputs>\\n{reference_outputs}\\n</reference_outputs>\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completeness.sonnet_3_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'claude-3-7-sonnet-bc180e0e' at:\n",
      "https://smith.langchain.com/o/e6a0f2b6-1ca3-5300-ba13-c04e59aa4f5b/datasets/6494c41f-c31f-48ae-b670-a8e8be84f9fb/compare?selectedSessions=bb569416-830a-4a1a-8127-b71d524a80c5\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [04:40, 280.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claude-3-7-sonnet evaluation completed!\n",
      "View the evaluation results for experiment: 'claude-3-haiku-fc0110a7' at:\n",
      "https://smith.langchain.com/o/e6a0f2b6-1ca3-5300-ba13-c04e59aa4f5b/datasets/c0e76c89-b219-4d87-a634-fa94850d0c7f/compare?selectedSessions=3ac35a17-a5bc-4b54-8295-c1146d9821ba\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [04:04, 244.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claude-3-haiku evaluation completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
    "import time\n",
    "\n",
    "def clean_streaming_response(raw_response: str) -> str:\n",
    "    \"\"\"스트리밍 응답에서 텍스트만 추출\"\"\"\n",
    "    import re\n",
    "    # {'type': 'text', 'text': '내용', 'index': 0} 패턴에서 text 부분만 추출\n",
    "    pattern = r\"'text': '([^']*?)'\"\n",
    "    matches = re.findall(pattern, raw_response)\n",
    "    clean_text = ''.join(matches)\n",
    "    # 이스케이프 문자 처리\n",
    "    clean_text = clean_text.replace('\\\\n', '\\n').replace('\\\\\\\\', '\\\\').replace(\"\\\\'\", \"'\")\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "def create_target_with_model(model, pricing_config):\n",
    "    \"\"\"특정 모델이 바인딩된 target 함수 생성\"\"\"\n",
    "    def model_specific_target(inputs: dict) -> dict:\n",
    "        # 원래 target 로직에 모델만 고정\n",
    "        start_time = time.time()\n",
    "        first_token_time = None\n",
    "        \n",
    "        stream = model.stream([\n",
    "            {\"role\": \"system\", \"content\": \"Answer the following question accurately\"},\n",
    "            {\"role\": \"user\", \"content\": inputs[\"question\"]}\n",
    "        ])\n",
    "        \n",
    "        full_response = \"\"\n",
    "        usage_metadata = None\n",
    "        response_metadata = None\n",
    "        \n",
    "        for chunk in stream:\n",
    "            if first_token_time is None and hasattr(chunk, 'content') and chunk.content:\n",
    "                first_token_time = time.time()\n",
    "            \n",
    "            if hasattr(chunk, 'content') and chunk.content:\n",
    "                if isinstance(chunk.content, str):\n",
    "                    full_response += chunk.content\n",
    "                elif isinstance(chunk.content, list):\n",
    "                    for item in chunk.content:\n",
    "                        if hasattr(item, 'text'):\n",
    "                            full_response += item.text\n",
    "                        else:\n",
    "                            full_response += str(item)\n",
    "            \n",
    "            if hasattr(chunk, 'usage_metadata') and chunk.usage_metadata:\n",
    "                usage_metadata = chunk.usage_metadata\n",
    "            \n",
    "            if hasattr(chunk, 'response_metadata') and chunk.response_metadata:\n",
    "                response_metadata = chunk.response_metadata\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        ttft_ms = (first_token_time - start_time) * 1000 if first_token_time else None\n",
    "        input_tokens = usage_metadata.get('input_tokens', 0) if usage_metadata else 0\n",
    "        output_tokens = usage_metadata.get('output_tokens', 0) if usage_metadata else 0\n",
    "        total_tokens = usage_metadata.get('total_tokens', 0) if usage_metadata else 0\n",
    "        \n",
    "        total_time_s = end_time - start_time\n",
    "        generation_time_s = end_time - first_token_time if first_token_time else total_time_s\n",
    "        tps = output_tokens / generation_time_s if generation_time_s > 0 and output_tokens > 0 else 0\n",
    "\n",
    "        input_cost = (input_tokens / 1000) * pricing_config[\"input_price_per_ktok\"]\n",
    "        output_cost = (output_tokens / 1000) * pricing_config[\"output_price_per_ktok\"]\n",
    "        total_cost = input_cost + output_cost\n",
    "\n",
    "        if \"{'type': 'text'\" in full_response:\n",
    "            full_response = clean_streaming_response(full_response)\n",
    "        \n",
    "        return { \n",
    "            \"answer\": full_response.strip(),\n",
    "            \"ttft_ms\": ttft_ms / 1000,\n",
    "            \"input_tokens\": input_tokens,\n",
    "            \"output_tokens\": output_tokens,\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"generation_time_s\": generation_time_s,\n",
    "            \"total_time_s\": total_time_s,\n",
    "            \"tps\": tps,\n",
    "            \"input_cost\": input_cost,\n",
    "            \"output_cost\": output_cost,\n",
    "            \"total_cost\": total_cost,\n",
    "\n",
    "        }\n",
    "    \n",
    "    return model_specific_target\n",
    "\n",
    "# 모델별로 순회하며 평가\n",
    "model_configs = [\n",
    "    {\n",
    "        \"name\": \"claude-3-7-sonnet\",\n",
    "        \"model\": ChatBedrockConverse(\n",
    "            model=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "            region_name=\"us-east-1\",\n",
    "            temperature=0,\n",
    "        ),\n",
    "        \"input_price_per_ktok\": 0.003,\n",
    "        \"output_price_per_ktok\": 0.015,\n",
    "        \"dataset_name\": \"blynx-dataset-sonnet-3-7\"\n",
    "        # \"dataset_name\": \"dummy-dataset-haiku-3-5\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"claude-3-haiku\",\n",
    "        \"model\": ChatBedrockConverse(\n",
    "            model=\"us.anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "            region_name=\"us-east-1\", \n",
    "            temperature=0,\n",
    "        ),\n",
    "        \"input_price_per_ktok\": 0.0008,\n",
    "        \"output_price_per_ktok\": 0.004,\n",
    "        \"dataset_name\": \"blynx-dataset-haiku-3-5\"\n",
    "        # \"dataset_name\": \"dummy-dataset-haiku-3-5\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for config in model_configs:\n",
    "    # 각 모델별 target 생성\n",
    "    model_target = create_target_with_model(config[\"model\"], config)\n",
    "    \n",
    "    # 후처리를 원한다면 파이프라인 구성\n",
    "    def add_model_info(result: dict) -> dict:\n",
    "        result[\"model_name\"] = config[\"name\"]\n",
    "        return result\n",
    "    \n",
    "    pipeline = RunnableLambda(model_target) | RunnableLambda(add_model_info)\n",
    "    \n",
    "    # 평가 실행\n",
    "    experiment_results = client.evaluate(\n",
    "        pipeline,\n",
    "        data=config[\"dataset_name\"],\n",
    "        evaluators=[\n",
    "            logical_coherence_evaluator,\n",
    "            faithfulness_evaluator,\n",
    "            following_instructions_evaluator,\n",
    "            completeness_evaluator,\n",
    "            correctness_evaluator,\n",
    "            helpfulness_evaluator,\n",
    "            professional_tone_evaluator,\n",
    "            relevance_evaluator,\n",
    "            stereotyping_evaluator,\n",
    "            harmfulness_evaluator,\n",
    "            refusal_evaluator,\n",
    "            tps_evaluator,\n",
    "            ttft_evaluator,\n",
    "            generation_time_evaluator,\n",
    "            input_tokens_evaluator,\n",
    "            output_tokens_evaluator,\n",
    "            cost_evaluator,\n",
    "        ],\n",
    "        experiment_prefix=f\"{config['name']}\",\n",
    "        max_concurrency=2,\n",
    "    )\n",
    "    \n",
    "    print(f\"{config['name']} evaluation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langsmith",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
